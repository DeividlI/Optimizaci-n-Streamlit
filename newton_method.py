import streamlit as st
import numpy as np
import math
import matplotlib.pyplot as plt
import pandas as pd

# --- Funciones Auxiliares (B√∫squeda de L√≠nea y Diferenciaci√≥n Num√©rica) ---

def w_to_x(w: float, a, b) -> float:
    return w * (b - a) + a

def busquedaDorada(funcion, epsilon: float, a: float = 0.0, b: float = 1.0) -> float:
    PHI = (1 + math.sqrt(5)) / 2 - 1
    aw, bw = 0, 1
    Lw = 1
    max_iter_dorada = 500
    k = 1
    w2 = bw - PHI * Lw
    w1 = aw + PHI * Lw
    fx2 = funcion(w_to_x(w2, a, b))
    fx1 = funcion(w_to_x(w1, a, b))

    while Lw > epsilon and k < max_iter_dorada:
        if fx1 > fx2:
            bw, w1, fx1 = w1, w2, fx2
            Lw = bw - aw
            w2 = bw - PHI * Lw
            fx2 = funcion(w_to_x(w2, a, b))
        else:
            aw, w2, fx2 = w2, w1, fx1
            Lw = bw - aw
            w1 = aw + PHI * Lw
            fx1 = funcion(w_to_x(w1, a, b))
        k += 1
    return (w_to_x(aw, a, b) + w_to_x(bw, a, b)) / 2

def gradiente(f, x, deltaX=1e-5):
    grad = np.zeros_like(x, dtype=float)
    for i in range(len(x)):
        xp = x.copy(); xp[i] += deltaX
        xn = x.copy(); xn[i] -= deltaX
        grad[i] = (f(xp) - f(xn)) / (2 * deltaX)
    return grad

def hessiano(f, x, deltaX=1e-4):
    n = len(x)
    hess = np.zeros((n, n), dtype=float)
    fx = f(x)
    for i in range(n):
        for j in range(n):
            if i == j:
                xp = x.copy(); xp[i] += deltaX
                xn = x.copy(); xn[i] -= deltaX
                hess[i, j] = (f(xp) - 2 * fx + f(xn)) / (deltaX**2)
            else:
                xpp = x.copy(); xpp[i] += deltaX; xpp[j] += deltaX
                xpn = x.copy(); xpn[i] += deltaX; xpn[j] -= deltaX
                xnp = x.copy(); xnp[i] -= deltaX; xnp[j] += deltaX
                xnn = x.copy(); xnn[i] -= deltaX; xnn[j] -= deltaX
                hess[i, j] = (f(xpp) - f(xpn) - f(xnp) + f(xnn)) / (4 * deltaX**2)
    return hess

def newton_algorithm(funcion, x0, epsilon1, epsilon2, M):
    xk = np.array(x0, dtype=float)
    historial = [xk]
    table_data = []
    termination_reason = "Se alcanz√≥ el m√°ximo n√∫mero de iteraciones."

    for k in range(M):
        grad_k = gradiente(funcion, xk)
        grad_norm = np.linalg.norm(grad_k)

        if grad_norm < epsilon1:
            termination_reason = f"Convergencia por gradiente peque√±o (||‚àáf|| < {epsilon1}) en la iteraci√≥n {k}."
            break

        hess_k = hessiano(funcion, xk)
        try:
            regularization = 1e-8 * np.identity(len(xk))
            hess_inv_k = np.linalg.inv(hess_k + regularization)
        except np.linalg.LinAlgError:
            termination_reason = "Error: La matriz Hessiana es singular. No se puede continuar."
            break
            
        pk = -np.dot(hess_inv_k, grad_k)
        
        direccion = "Newton"
        if np.dot(pk, grad_k) > 0:
            pk = -grad_k
            direccion = "Descenso de Gradiente (fallback)"

        def alpha_funcion(alpha): return funcion(xk + alpha * pk)
        alpha_optimo = busquedaDorada(alpha_funcion, epsilon2)
        
        x_k1 = xk + alpha_optimo * pk
        historial.append(x_k1)
        
        table_data.append({
            'Iteraci√≥n (k)': k,
            'xk': f"[{xk[0]:.4f}, {xk[1]:.4f}]",
            '||‚àáf(xk)||': grad_norm,
            'Œ±*': alpha_optimo,
            'Direcci√≥n': direccion,
            'xk+1': f"[{x_k1[0]:.4f}, {x_k1[1]:.4f}]"
        })
        
        if np.linalg.norm(x_k1 - xk) / (np.linalg.norm(xk) + 1e-8) < epsilon2 and k > 0:
            termination_reason = f"Convergencia por cambio relativo peque√±o en la iteraci√≥n {k+1}."
            xk = x_k1
            break
            
        xk = x_k1
    
    return xk, historial, table_data, termination_reason

def plot_newton_results(ax, func, bounds, history):
    x_min, x_max = bounds[0]; y_min, y_max = bounds[1]
    rango_x = np.linspace(x_min, x_max, 200)
    rango_y = np.linspace(y_min, y_max, 200)
    X, Y = np.meshgrid(rango_x, rango_y)
    Z = func([X, Y])

    ax.contourf(X, Y, Z, levels=50, cmap='viridis', alpha=0.9)
    ruta = np.array(history)
    ax.plot(ruta[:, 0], ruta[:, 1], 'o-', color='red', markersize=4, linewidth=1.5, label='Ruta de Optimizaci√≥n')
    ax.plot(ruta[0, 0], ruta[0, 1], 'o', color='lime', markersize=8, label='Inicio')
    ax.plot(ruta[-1, 0], ruta[-1, 1], '*', color='yellow', markersize=12, label='Fin (Soluci√≥n)')
    
    ax.set_title("Ruta de Optimizaci√≥n con M√©todo de Newton", fontsize=16)
    ax.set_xlabel('x_0'); ax.set_ylabel('x_1')
    ax.legend(); ax.grid(True, linestyle='--', alpha=0.6)

def show_newton_method(FUNCIONES, evaluar_funcion):
    st.markdown("## üî¢ M√©todo de Newton")
    st.markdown("""
    El M√©todo de Newton es un algoritmo de **segundo orden**, lo que significa que utiliza no solo el gradiente (primera derivada) sino tambi√©n la **matriz Hessiana** (segunda derivada) para modelar la funci√≥n como una superficie cuadr√°tica en cada paso.
    """)

    with st.expander("Configuraci√≥n del Algoritmo y la Funci√≥n", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            funcion_seleccionada = st.selectbox("üéØ Selecciona la funci√≥n:", list(FUNCIONES.keys()), key="newton_func")
            info_funcion = FUNCIONES[funcion_seleccionada]
            
            st.markdown("**Par√°metros de Parada (Tolerancias):**")
            epsilon1 = st.number_input("Œµ‚ÇÅ (Tolerancia de ||‚àáf||)", 0.0, 1.0, 1e-5, format="%.6f")
            epsilon2 = st.number_input("Œµ‚ÇÇ (Tolerancia de paso)", 0.0, 1.0, 1e-5, format="%.6f")

        with col2:
            st.latex(info_funcion['latex'])
            st.markdown("**Par√°metros de Ejecuci√≥n:**")
            max_iter = st.slider("M (M√°ximo de iteraciones)", 10, 200, 100, 5)
            st.markdown("**üìç Punto Inicial (x‚ÇÄ):**")
            default_x0 = info_funcion.get("x0", np.array([0.0, 0.0]))
            x0_cols = st.columns(len(default_x0))
            x0_vals = [c.number_input(f'x0_{i}', value=float(default_x0[i]), key=f'newton_x0_{i}') for i, c in enumerate(x0_cols)]
            x0 = np.array(x0_vals)

    if st.button("üöÄ Ejecutar M√©todo de Newton", key="run_newton"):
        func_objetivo = lambda x: evaluar_funcion(x, funcion_seleccionada)
        
        with st.spinner("Calculando gradientes y hessianos..."):
            solucion, historial, table_data, reason = newton_algorithm(
                func_objetivo, x0, epsilon1, epsilon2, max_iter
            )
        
        st.success("‚úÖ Optimizaci√≥n completada!")
        
        col1, col2 = st.columns(2)
        col1.metric("üéØ Soluci√≥n Encontrada (x*)", f"[{solucion[0]:.5f}, {solucion[1]:.5f}]")
        col2.metric("üìä Valor √ìptimo f(x*)", f"{func_objetivo(solucion):.6f}")
        st.info(f"Raz√≥n de la parada: {reason}")

        st.markdown("### üìà Visualizaci√≥n de la Optimizaci√≥n")
        fig, ax = plt.subplots(figsize=(10, 8))
        plot_newton_results(ax, func_objetivo, info_funcion["intervalos"], historial)
        st.pyplot(fig)
        
        st.markdown("### üìã Tabla de Iteraciones")
        st.dataframe(pd.DataFrame(table_data), use_container_width=True)

        st.markdown("### üìò Informaci√≥n adicional: M√©todo de Newton")
        st.markdown("""
        El **M√©todo de Newton** es un algoritmo de optimizaci√≥n de segundo orden que utiliza derivadas tanto de primer orden (gradientes) como de segundo orden (Hessianas) para encontrar puntos √≥ptimos (m√≠nimos o m√°ximos locales) de una funci√≥n.

        üîç **Resumen de sus caracter√≠sticas clave:**
        - Utiliza una **aproximaci√≥n cuadr√°tica** de la funci√≥n para buscar el m√≠nimo.
        - Calcula una direcci√≥n de b√∫squeda resolviendo: $H(x_k) p_k = -\\nabla f(x_k)$
        - Realiza una **b√∫squeda unidireccional** para encontrar el mejor paso Œ± en esa direcci√≥n.
        - Tiene **alta velocidad de convergencia**, especialmente si el punto inicial est√° cerca del √≥ptimo.
        - Puede fallar si la Hessiana es **singular o indefinida**, o si el punto inicial est√° lejos del m√≠nimo.

        üß† **Fundamento te√≥rico (seg√∫n tus diapositivas):**
        - Forma parte de los **m√©todos num√©ricos**, que construyen soluciones iterativas mejoradas.
        - Se apoya en los **m√©todos basados en gradiente**, que explotan la informaci√≥n derivada.
        - La Hessiana y el gradiente pueden calcularse **num√©ricamente** (por diferencia central).
        - Si el **determinante de la Hessiana es > 0** y $f_{xx}>0$, hay un m√≠nimo local.

        üìé **Ventajas y desventajas:**
        - ‚úÖ Muy eficiente en funciones suaves y bien condicionadas.
        - ‚ùå Computacionalmente costoso por la inversi√≥n de matrices.
        - ‚ùå No garantiza siempre el descenso, se puede necesitar fallback como el gradiente descendente.

        üìñ **Aplicaciones t√≠picas**: Ingenier√≠a, Machine Learning, Dise√±o √≥ptimo, Modelado de sistemas continuos.

        üìö Basado en: *Optimizaci√≥n - clase introductoria*, curso de Ad√°n E. Aguilar y libro de Deb Kalyanmoy.
        """)
